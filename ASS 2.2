1)HDFS:
*) HDFS is the primary distributed storage used by Hadoop applications. 
*) A HDFS cluster primarily consists of a NameNode that manages the file system metadata and DataNodes that store the actual data. 

2)HADOOP CLUSTER:

              * A Hadoop cluster is a special type of computational cluster designed specifically for storing and analyzing huge amounts of unstructured data in a distributed computing environment. 
              * Hadoop clusters are known for boosting the speed of data analysis applications. 
              * They also are highly scalable: If a cluster's processing power is overwhelmed by growing volumes of data, additional cluster nodes can be added to increase throughput.
              * Hadoop clusters also are highly resistant to failure because each piece of data is copied onto other cluster nodes, which ensures that the data is not lost if one node fails.
              * Hadoop clusters are often referred to as "shared nothing" systems because the only thing that is shared between nodes is the network that connects them.
              
              Hadoop cluster has 3 components:
                1) Client
                2) Master
                3) Slave
              
 1)Client: 
                It is neither master nor slave, rather play a role of loading the data into cluster, submit MapReduce jobs describing how the data should be processed and then retrieve the data to see the response after job completion. 
 2)Master:
            The Masters consists of 3 components NameNode, Secondary Node name and JobTracker. 
   Name Node:
              NameNode does NOT store the files but only the file's metadata.
   JobTracker:
              JobTracker coordinates the parallel processing of data using MapReduce.
   Secondary Name Node:
              The job of Secondary Node is to contact NameNode in a periodic manner after certain time interval(by default 1 hour). 
NameNode which keeps all filesystem metadata in RAM has no capability to process that metadata on to disk.
              
3)Slaves:
              Slave nodes are the majority of machines in Hadoop Cluster and are responsible to
                * Store the data
                * Process the computation
                Each slave runs both a DataNode and Task Tracker daemon which communicates to their masters. The Task Tracker daemon is a slave to the JobTracker and the DataNode daemon a slave to the NameNode.

                      
3)HDFS BLOCKS:
                *  Block is a physical division of datafile done by HDFS while storing data.
                *  Hadoop distributed file system stores the data in terms of blocks. 
                *  However the block size in HDFS is very large. 
                *  The default size of HDFS block is 64MB. 
                *  The files are split into 64MB blocks and then stored into the hadoop filesystem. 
                *  The hadoop application is responsible for distributing the data blocks across multiple nodes. 
                
Advantages:
* The blocks are of fixed size, so it is very easy to calculate the number of blocks that can be stored on a disk.
* HDFS block concept simplifies the storage of the datanodes. The datanodes doesnâ€™t need to concern about the blocks metadata data like file permissions etc. The namenode maintains the metadata of all the blocks.
* If the size of the file is less than the HDFS block size, then the file does not occupy the complete block storage.
* As the file is chunked into blocks, it is easy to store a file that is larger than the disk size as the data blocks are distributed and stored on multiple nodes in a hadoop cluster.
* Blocks are easy to replicate between the datanodes and thus provide fault tolerance and high availability. Hadoop framework replicates each block across multiple nodes (default replication factor is 3). In case of any node failure or block corruption, the same block can be read from another node.
