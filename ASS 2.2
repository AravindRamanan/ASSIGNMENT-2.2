1)HDFS:
* HDFS is the primary distributed storage used by Hadoop applications. 
* A HDFS cluster primarily consists of a NameNode that manages the file system metadata and DataNodes that store the actual data.

HDFS follows the master-slave architecture and it has the following elements.

1)Namenode:
The namenode is the commodity hardware that contains the GNU/Linux operating system and the namenode software. It is a software that can be run on commodity hardware. The system having the namenode acts as the master server and it does the following tasks:

        * Manages the file system namespace.
        * Regulates client’s access to files.
        * It also executes file system operations such as renaming, closing, and opening files and directories.
2)Datanode:
The datanode is a commodity hardware having the GNU/Linux operating system and datanode software. For every node (Commodity hardware/System) in a cluster, there will be a datanode. These nodes manage the data storage of their system.

        * Datanodes perform read-write operations on the file systems, as per client request.
        * They also perform operations such as block creation, deletion, and replication according to the instructions of the namenode.

Goals of HDFS:

* Fault detection and recovery : Since HDFS includes a large number of commodity hardware, failure of components is frequent. Therefore HDFS should have mechanisms for quick and automatic fault detection and recovery.

* Huge datasets : HDFS should have hundreds of nodes per cluster to manage the applications having huge datasets.

* Hardware at data : A requested task can be done efficiently, when the computation takes place near the data. Especially where huge datasets are involved, it reduces the network traffic and increases the throughput.

2)HADOOP CLUSTER:

              * A Hadoop cluster is a special type of computational cluster designed specifically for storing and analyzing huge amounts of unstructured data in a distributed computing environment. 
              * Hadoop clusters are known for boosting the speed of data analysis applications. 
              * They also are highly scalable: If a cluster's processing power is overwhelmed by growing volumes of data, additional cluster nodes can be added to increase throughput.
              * Hadoop clusters also are highly resistant to failure because each piece of data is copied onto other cluster nodes, which ensures that the data is not lost if one node fails.
              * Hadoop clusters are often referred to as "shared nothing" systems because the only thing that is shared between nodes is the network that connects them.
              
              Hadoop cluster has 3 components:
                1) Client
                2) Master
                3) Slave
              
 1)Client: 
                It is neither master nor slave, rather play a role of loading the data into cluster, submit MapReduce jobs describing how the data should be processed and then retrieve the data to see the response after job completion. 
 2)Master:
            The Masters consists of 3 components NameNode, Secondary Node name and JobTracker. 
   Name Node:
              NameNode does NOT store the files but only the file's metadata.
   JobTracker:
              JobTracker coordinates the parallel processing of data using MapReduce.
   Secondary Name Node:
              The job of Secondary Node is to contact NameNode in a periodic manner after certain time interval(by default 1 hour). 
NameNode which keeps all filesystem metadata in RAM has no capability to process that metadata on to disk.
              
3)Slaves:
              Slave nodes are the majority of machines in Hadoop Cluster and are responsible to
                * Store the data
                * Process the computation
                Each slave runs both a DataNode and Task Tracker daemon which communicates to their masters. The Task Tracker daemon is a slave to the JobTracker and the DataNode daemon a slave to the NameNode.

                      
3)HDFS BLOCKS:
                *  Block is a physical division of datafile done by HDFS while storing data.
                *  Hadoop distributed file system stores the data in terms of blocks. 
                *  However the block size in HDFS is very large. 
                *  The default size of HDFS block is 64MB. 
                *  The files are split into 64MB blocks and then stored into the hadoop filesystem. 
                *  The hadoop application is responsible for distributing the data blocks across multiple nodes. 
                
Advantages:
* The blocks are of fixed size, so it is very easy to calculate the number of blocks that can be stored on a disk.
* HDFS block concept simplifies the storage of the datanodes. The datanodes doesn’t need to concern about the blocks metadata data like file permissions etc. The namenode maintains the metadata of all the blocks.
* If the size of the file is less than the HDFS block size, then the file does not occupy the complete block storage.
* As the file is chunked into blocks, it is easy to store a file that is larger than the disk size as the data blocks are distributed and stored on multiple nodes in a hadoop cluster.
* Blocks are easy to replicate between the datanodes and thus provide fault tolerance and high availability. Hadoop framework replicates each block across multiple nodes (default replication factor is 3). In case of any node failure or block corruption, the same block can be read from another node.
